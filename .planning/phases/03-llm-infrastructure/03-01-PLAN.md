---
phase: 03-llm-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - litellm-config.yaml
  - src/shared/lib/env.ts
  - src/shared/db/schema/organizations.ts
  - src/features/llm/llm.client.ts
  - src/features/llm/llm.service.ts
  - src/features/llm/llm.types.ts
  - src/features/llm/index.ts
autonomous: true

user_setup:
  - service: litellm
    why: "LLM model abstraction proxy"
    env_vars:
      - name: LITELLM_MASTER_KEY
        source: "Generate a secure random string (32+ chars)"
      - name: LITELLM_SALT_KEY
        source: "Generate a secure random string (32+ chars)"
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API keys (if using OpenAI models)"
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API keys (if using Claude models)"

must_haves:
  truths:
    - "LiteLLM container starts and responds to health checks"
    - "Backend can send chat completions via OpenAI SDK to LiteLLM"
    - "Organization has configurable model preference"
    - "LLM service returns typed chat completion responses"
  artifacts:
    - path: "docker-compose.yml"
      provides: "LiteLLM service definition"
      contains: "litellm:"
    - path: "litellm-config.yaml"
      provides: "Model configuration for LiteLLM"
      contains: "model_list:"
    - path: "src/features/llm/llm.client.ts"
      provides: "OpenAI SDK wrapper for LiteLLM"
      exports: ["createLLMClient"]
    - path: "src/features/llm/llm.service.ts"
      provides: "Chat completion and embedding functions"
      exports: ["chatCompletion", "generateEmbedding"]
  key_links:
    - from: "src/features/llm/llm.client.ts"
      to: "http://litellm:4000"
      via: "OpenAI SDK baseURL"
      pattern: "baseURL.*LITELLM_URL"
    - from: "src/features/llm/llm.service.ts"
      to: "src/features/llm/llm.client.ts"
      via: "createLLMClient import"
      pattern: "import.*createLLMClient"
---

<objective>
Set up LiteLLM proxy for model abstraction and create the LLM client layer.

Purpose: Enable swappable LLM providers (Claude, GPT, local models) via a single OpenAI-compatible interface. This is the foundation for all AI features - chat, embeddings, and memory.

Output: LiteLLM Docker container running as sidecar, OpenAI SDK client wrapper pointing at LiteLLM, organization model preference field, and typed LLM service functions.
</objective>

<execution_context>
@/Users/dio/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dio/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-infrastructure/03-RESEARCH.md
@.planning/phases/03-llm-infrastructure/03-CONTEXT.md
@src/shared/lib/env.ts
@src/shared/db/schema/organizations.ts
@docker-compose.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LiteLLM to Docker Compose</name>
  <files>
    docker-compose.yml
    litellm-config.yaml
    src/shared/lib/env.ts
  </files>
  <action>
1. Create `litellm-config.yaml` in project root with model configuration:
   - gpt-4o model (openai/gpt-4o)
   - claude-3-5-sonnet model (anthropic/claude-3-5-sonnet-20241022)
   - text-embedding-3-small model (openai/text-embedding-3-small)
   - Disable verbose logging, enable JSON logs
   - Set request_timeout: 600

2. Add litellm service to docker-compose.yml:
   - Image: ghcr.io/berriai/litellm:main-v1.55.0
   - Port: 4000:4000
   - Mount litellm-config.yaml to /app/config.yaml
   - Environment: LITELLM_MASTER_KEY, LITELLM_SALT_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY from host env
   - Command: ["--config", "/app/config.yaml", "--port", "4000"]
   - Healthcheck: curl http://localhost:4000/health/liveliness
   - Depends on: postgres (for potential future key management)

3. Update src/shared/lib/env.ts to add new env vars:
   - LITELLM_URL: z.string().url().default('http://localhost:4000')
   - LITELLM_API_KEY: z.string().min(1) (this is the master key)
   - OPENAI_API_KEY: z.string().optional() (may not be set if using Anthropic only)
   - ANTHROPIC_API_KEY: z.string().optional()

Note: Use .optional() for provider API keys since users may only configure one provider.
  </action>
  <verify>
    docker compose up -d litellm && sleep 5 && curl -s http://localhost:4000/health/liveliness | grep -q "healthy"
  </verify>
  <done>LiteLLM container running and responding to health checks</done>
</task>

<task type="auto">
  <name>Task 2: Create LLM Client and Service</name>
  <files>
    src/features/llm/llm.types.ts
    src/features/llm/llm.client.ts
    src/features/llm/llm.service.ts
    src/features/llm/index.ts
    package.json
  </files>
  <action>
1. Install OpenAI SDK: `bun add openai`

2. Create src/features/llm/llm.types.ts:
   - ChatMessage type (role: 'system' | 'user' | 'assistant', content: string)
   - ChatCompletionOptions (model?: string, stream?: boolean, temperature?: number, maxTokens?: number)
   - ChatCompletionResult (content: string, model: string, usage: { promptTokens, completionTokens, totalTokens })
   - EmbeddingResult (embedding: number[], model: string, dimensions: number)

3. Create src/features/llm/llm.client.ts:
   - Import OpenAI from 'openai' and env
   - Export createLLMClient() function that returns new OpenAI({ apiKey: env.LITELLM_API_KEY, baseURL: env.LITELLM_URL })
   - Client is created per-request, not singleton (allows model-specific configuration)

4. Create src/features/llm/llm.service.ts:
   - chatCompletion(messages: ChatMessage[], options?: ChatCompletionOptions): Promise&lt;ChatCompletionResult&gt;
     - Default model: 'gpt-4o'
     - Handle both streaming and non-streaming (for now, implement non-streaming only)
     - Map OpenAI response to ChatCompletionResult type
   - generateEmbedding(text: string, model?: string): Promise&lt;EmbeddingResult&gt;
     - Default model: 'text-embedding-3-small'
     - Clean text (replace newlines with spaces)
     - Return embedding array with model info

5. Create src/features/llm/index.ts barrel export:
   - Export types, client, and service functions

Pattern reference: See RESEARCH.md Pattern 1 (LiteLLM Client Abstraction)
  </action>
  <verify>
    bun run typecheck && bun test src/features/llm/
  </verify>
  <done>LLM client and service functions implemented with TypeScript types, typecheck passes</done>
</task>

<task type="auto">
  <name>Task 3: Add Organization Model Preference</name>
  <files>
    src/shared/db/schema/organizations.ts
    src/features/llm/llm.service.ts
  </files>
  <action>
1. Update src/shared/db/schema/organizations.ts:
   - Add `llmModel` column: text('llm_model').default('gpt-4o')
   - Add `llmFallbackModel` column: text('llm_fallback_model').default('claude-3-5-sonnet')
   - These store the organization's preferred primary and fallback models

2. Generate and run migration:
   - Run `bun run db:generate` to create migration
   - Run `bun run db:migrate` to apply

3. Update src/features/llm/llm.service.ts:
   - Add chatCompletionForOrg(organizationId: string, messages: ChatMessage[], options?: ChatCompletionOptions)
   - This function fetches org's llmModel preference, uses it as default
   - If primary fails with non-retryable error, try fallbackModel once
   - Return result with model actually used

Per CONTEXT.md: "Single fallback - Primary fails -> try fallback model once, then error"
  </action>
  <verify>
    bun run db:migrate && bun run typecheck
  </verify>
  <done>Organizations have configurable LLM model preference with single fallback, migration applied</done>
</task>

</tasks>

<verification>
1. LiteLLM health: `curl http://localhost:4000/health/liveliness`
2. TypeScript: `bun run typecheck` passes
3. Docker: `docker compose ps` shows litellm healthy
4. Schema: Database has llm_model and llm_fallback_model columns on organizations
</verification>

<success_criteria>
- LiteLLM container starts and passes health checks
- OpenAI SDK client configured to point at LiteLLM
- chatCompletion and generateEmbedding functions work
- Organizations have llmModel and llmFallbackModel columns
- chatCompletionForOrg uses org's model preference
- All TypeScript types properly defined
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-infrastructure/03-01-SUMMARY.md`
</output>
