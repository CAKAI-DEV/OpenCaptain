---
phase: 03-llm-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/shared/db/schema/embeddings.ts
  - src/shared/db/schema/index.ts
  - src/features/rag/rag.types.ts
  - src/features/rag/rag.chunker.ts
  - src/features/rag/rag.service.ts
  - src/features/rag/index.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "pgvector extension is enabled in PostgreSQL"
    - "Embeddings can be stored with vector column"
    - "Documents can be chunked for indexing"
    - "Similar documents can be retrieved by vector similarity"
    - "RAG queries respect project visibility"
  artifacts:
    - path: "src/shared/db/schema/embeddings.ts"
      provides: "Embeddings table with vector column"
      contains: "vector('embedding'"
    - path: "src/features/rag/rag.chunker.ts"
      provides: "Text splitting utilities"
      exports: ["chunkDocument"]
    - path: "src/features/rag/rag.service.ts"
      provides: "Embedding storage and retrieval"
      exports: ["indexDocument", "findSimilarDocuments"]
  key_links:
    - from: "src/features/rag/rag.service.ts"
      to: "src/shared/db/schema/embeddings.ts"
      via: "Drizzle query"
      pattern: "db.*embeddings"
    - from: "src/features/rag/rag.service.ts"
      to: "cosineDistance"
      via: "pgvector similarity"
      pattern: "cosineDistance.*embedding"
---

<objective>
Set up pgvector for vector similarity search and create the RAG pipeline.

Purpose: Enable semantic search over project documents and conversations. RAG retrieval provides relevant context to the LLM for informed responses. This is critical for AI-01 (project context/history knowledge).

Output: pgvector extension enabled, embeddings table with HNSW index, document chunking utilities, and RAG retrieval service that respects visibility rules.
</objective>

<execution_context>
@/Users/dio/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dio/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-infrastructure/03-RESEARCH.md
@.planning/phases/03-llm-infrastructure/03-CONTEXT.md
@src/shared/db/index.ts
@src/shared/db/schema/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enable pgvector and Create Embeddings Schema</name>
  <files>
    src/shared/db/schema/embeddings.ts
    src/shared/db/schema/index.ts
    package.json
  </files>
  <action>
1. Install pgvector package: `bun add pgvector`

2. Create SQL migration to enable pgvector extension:
   - Create file: drizzle/0007_enable_pgvector.sql (adjust number based on existing migrations)
   - Content: `CREATE EXTENSION IF NOT EXISTS vector;`
   - Run manually or add to migration sequence

3. Create src/shared/db/schema/embeddings.ts:
   ```typescript
   import { index, pgTable, text, uuid, vector, timestamp, jsonb } from 'drizzle-orm/pg-core';
   import { organizations } from './organizations';
   import { projects } from './projects';
   import { users } from './users';

   export const embeddings = pgTable(
     'embeddings',
     {
       id: uuid('id').primaryKey().defaultRandom(),
       content: text('content').notNull(),
       embedding: vector('embedding', { dimensions: 1536 }),
       sourceType: text('source_type').notNull(),  // 'document', 'conversation', 'memory'
       sourceId: uuid('source_id').notNull(),
       organizationId: uuid('organization_id').notNull().references(() => organizations.id),
       projectId: uuid('project_id').references(() => projects.id),
       userId: uuid('user_id').references(() => users.id),
       embeddingModel: text('embedding_model').notNull().default('text-embedding-3-small'),
       metadata: jsonb('metadata'),  // Additional context (chunk index, etc.)
       createdAt: timestamp('created_at', { withTimezone: true }).defaultNow().notNull(),
     },
     (table) => [
       index('embeddings_hnsw_idx').using('hnsw', table.embedding.op('vector_cosine_ops')),
       index('embeddings_org_project_idx').on(table.organizationId, table.projectId),
       index('embeddings_source_idx').on(table.sourceType, table.sourceId),
     ]
   );

   export type Embedding = typeof embeddings.$inferSelect;
   export type NewEmbedding = typeof embeddings.$inferInsert;
   ```

4. Update src/shared/db/schema/index.ts to export embeddings

5. Generate and run migration:
   - First manually run: `psql -h localhost -p 5433 -U blockbot -d blockbot -c "CREATE EXTENSION IF NOT EXISTS vector;"`
   - Then: `bun run db:generate`
   - Then: `bun run db:migrate`

Pattern reference: See RESEARCH.md Pattern 2 (Vector Column with Drizzle)
Note: HNSW index should be created after initial data load for best performance, but creating it empty is fine for development.
  </action>
  <verify>
    bun run db:migrate && psql -h localhost -p 5433 -U blockbot -d blockbot -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
  </verify>
  <done>pgvector extension enabled, embeddings table created with HNSW index</done>
</task>

<task type="auto">
  <name>Task 2: Create Document Chunking Utilities</name>
  <files>
    src/features/rag/rag.types.ts
    src/features/rag/rag.chunker.ts
    package.json
  </files>
  <action>
1. Install text splitter: `bun add @langchain/textsplitters`

2. Create src/features/rag/rag.types.ts:
   ```typescript
   export type ChunkMetadata = {
     chunkIndex: number;
     totalChunks: number;
     sourceType: string;
     sourceId: string;
   };

   export type DocumentChunk = {
     content: string;
     metadata: ChunkMetadata;
   };

   export type IndexDocumentInput = {
     content: string;
     sourceType: 'document' | 'conversation' | 'memory';
     sourceId: string;
     organizationId: string;
     projectId?: string;
     userId?: string;
     metadata?: Record<string, unknown>;
   };

   export type SimilarDocument = {
     id: string;
     content: string;
     sourceType: string;
     sourceId: string;
     similarity: number;
     metadata: Record<string, unknown> | null;
   };
   ```

3. Create src/features/rag/rag.chunker.ts:
   ```typescript
   import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
   import type { DocumentChunk, ChunkMetadata } from './rag.types';

   const DEFAULT_CHUNK_SIZE = 400;
   const DEFAULT_CHUNK_OVERLAP = 50;

   export async function chunkDocument(
     content: string,
     sourceType: string,
     sourceId: string,
     options?: { chunkSize?: number; chunkOverlap?: number }
   ): Promise<DocumentChunk[]> {
     const splitter = new RecursiveCharacterTextSplitter({
       chunkSize: options?.chunkSize ?? DEFAULT_CHUNK_SIZE,
       chunkOverlap: options?.chunkOverlap ?? DEFAULT_CHUNK_OVERLAP,
       separators: ['\n\n', '\n', '. ', ' ', ''],
     });

     const chunks = await splitter.splitText(content);

     return chunks.map((chunk, index) => ({
       content: chunk,
       metadata: {
         chunkIndex: index,
         totalChunks: chunks.length,
         sourceType,
         sourceId,
       },
     }));
   }
   ```

Per RESEARCH.md: Start with 400 tokens, 50 overlap; tune based on retrieval quality.
  </action>
  <verify>
    bun run typecheck
  </verify>
  <done>Document chunking utilities created with configurable chunk size</done>
</task>

<task type="auto">
  <name>Task 3: Create RAG Service with Visibility Filtering</name>
  <files>
    src/features/rag/rag.service.ts
    src/features/rag/index.ts
  </files>
  <action>
1. Create src/features/rag/rag.service.ts:
   ```typescript
   import { and, cosineDistance, desc, gt, inArray, sql } from 'drizzle-orm';
   import { db } from '../../shared/db';
   import { embeddings } from '../../shared/db/schema';
   import { generateEmbedding } from '../llm';
   import { chunkDocument } from './rag.chunker';
   import type { IndexDocumentInput, SimilarDocument } from './rag.types';

   const SIMILARITY_THRESHOLD = 0.7;
   const DEFAULT_LIMIT = 5;

   export async function indexDocument(input: IndexDocumentInput): Promise<string[]> {
     const chunks = await chunkDocument(input.content, input.sourceType, input.sourceId);
     const insertedIds: string[] = [];

     for (const chunk of chunks) {
       const embeddingResult = await generateEmbedding(chunk.content);

       const [inserted] = await db.insert(embeddings).values({
         content: chunk.content,
         embedding: embeddingResult.embedding,
         sourceType: input.sourceType,
         sourceId: input.sourceId,
         organizationId: input.organizationId,
         projectId: input.projectId,
         userId: input.userId,
         embeddingModel: embeddingResult.model,
         metadata: { ...input.metadata, ...chunk.metadata },
       }).returning({ id: embeddings.id });

       insertedIds.push(inserted.id);
     }

     return insertedIds;
   }

   export async function findSimilarDocuments(
     query: string,
     visibleProjectIds: string[],
     options?: { limit?: number; threshold?: number }
   ): Promise<SimilarDocument[]> {
     const queryEmbedding = await generateEmbedding(query);
     const limit = options?.limit ?? DEFAULT_LIMIT;
     const threshold = options?.threshold ?? SIMILARITY_THRESHOLD;

     // Similarity = 1 - cosine_distance (higher is more similar)
     const similarity = sql<number>`1 - (${cosineDistance(embeddings.embedding, queryEmbedding.embedding)})`;

     // Pre-filter by project visibility, then sort by similarity
     // Use larger candidate set to avoid post-filter issues (see RESEARCH.md Pitfall 1)
     const results = await db
       .select({
         id: embeddings.id,
         content: embeddings.content,
         sourceType: embeddings.sourceType,
         sourceId: embeddings.sourceId,
         similarity,
         metadata: embeddings.metadata,
       })
       .from(embeddings)
       .where(
         and(
           visibleProjectIds.length > 0
             ? inArray(embeddings.projectId, visibleProjectIds)
             : undefined,
           gt(similarity, threshold)
         )
       )
       .orderBy(desc(similarity))
       .limit(limit);

     return results;
   }

   export async function deleteEmbeddingsBySource(
     sourceType: string,
     sourceId: string
   ): Promise<number> {
     const result = await db
       .delete(embeddings)
       .where(
         and(
           sql`${embeddings.sourceType} = ${sourceType}`,
           sql`${embeddings.sourceId} = ${sourceId}`
         )
       );

     return result.rowCount ?? 0;
   }
   ```

2. Create src/features/rag/index.ts barrel export:
   ```typescript
   export * from './rag.types';
   export { chunkDocument } from './rag.chunker';
   export { indexDocument, findSimilarDocuments, deleteEmbeddingsBySource } from './rag.service';
   ```

CRITICAL per RESEARCH.md Pitfall 5: Always filter by user's visible project IDs. The visibleProjectIds parameter comes from the visibility middleware established in Phase 2.

Per CONTEXT.md: "Hybrid - Shared project index + per-user visibility metadata for filtering"
  </action>
  <verify>
    bun run typecheck
  </verify>
  <done>RAG service implemented with visibility-aware retrieval and document indexing</done>
</task>

</tasks>

<verification>
1. pgvector: `psql -c "SELECT * FROM pg_extension WHERE extname = 'vector';"` shows vector extension
2. Schema: embeddings table exists with vector column and HNSW index
3. TypeScript: `bun run typecheck` passes
4. Index check: `psql -c "\d embeddings"` shows embedding column and indexes
</verification>

<success_criteria>
- pgvector extension enabled in PostgreSQL
- embeddings table has vector(1536) column for OpenAI embeddings
- HNSW index created for fast similarity search
- chunkDocument splits text with configurable size/overlap
- indexDocument generates embeddings and stores with visibility metadata
- findSimilarDocuments retrieves by vector similarity with project filtering
- All queries respect visibility by requiring visibleProjectIds parameter
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-infrastructure/03-02-SUMMARY.md`
</output>
