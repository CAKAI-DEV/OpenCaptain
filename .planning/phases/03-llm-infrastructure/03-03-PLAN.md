---
phase: 03-llm-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/shared/db/schema/conversations.ts
  - src/shared/db/schema/memories.ts
  - src/shared/db/schema/index.ts
  - src/shared/lib/queue/client.ts
  - src/shared/lib/queue/workers.ts
  - src/shared/lib/queue/index.ts
  - src/features/memory/memory.types.ts
  - src/features/memory/memory.service.ts
  - src/features/memory/memory.worker.ts
  - src/features/memory/index.ts
  - src/features/conversations/conversations.types.ts
  - src/features/conversations/conversations.service.ts
  - src/features/conversations/conversations.routes.ts
  - src/features/conversations/index.ts
  - src/index.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Conversations are stored with message history"
    - "Memory is hierarchical (org/project/user scope)"
    - "Old messages are summarized to semantic memory"
    - "Agent can retrieve relevant context for conversations"
    - "Memory persists across sessions"
  artifacts:
    - path: "src/shared/db/schema/conversations.ts"
      provides: "Conversation and message tables"
      contains: "conversationMessages"
    - path: "src/shared/db/schema/memories.ts"
      provides: "Hierarchical memory table"
      contains: "memories"
    - path: "src/features/memory/memory.service.ts"
      provides: "Memory CRUD operations"
      exports: ["storeMemory", "retrieveMemories"]
    - path: "src/features/conversations/conversations.service.ts"
      provides: "Conversation handling with context assembly"
      exports: ["createConversation", "addMessage", "getConversationContext"]
  key_links:
    - from: "src/features/conversations/conversations.service.ts"
      to: "src/features/rag/rag.service.ts"
      via: "RAG retrieval for context"
      pattern: "findSimilarDocuments"
    - from: "src/features/conversations/conversations.service.ts"
      to: "src/features/llm/llm.service.ts"
      via: "LLM chat completion"
      pattern: "chatCompletionForOrg"
    - from: "src/features/memory/memory.worker.ts"
      to: "src/features/llm/llm.service.ts"
      via: "Summarization"
      pattern: "chatCompletion"
---

<objective>
Create the memory management system and conversation handling with context assembly.

Purpose: Enable AI-02 (memory persistence across sessions) and complete AI-01 (project context knowledge). Memory consolidation ensures conversations don't exceed context limits while preserving important information.

Output: Hierarchical memory tables, conversation storage, BullMQ worker for memory consolidation, and conversation service that assembles context from memory + RAG.
</objective>

<execution_context>
@/Users/dio/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dio/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-infrastructure/03-RESEARCH.md
@.planning/phases/03-llm-infrastructure/03-CONTEXT.md
@.planning/phases/03-llm-infrastructure/03-01-SUMMARY.md
@.planning/phases/03-llm-infrastructure/03-02-SUMMARY.md
@src/shared/db/schema/index.ts
@src/features/llm/index.ts
@src/features/rag/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Conversation and Memory Schemas</name>
  <files>
    src/shared/db/schema/conversations.ts
    src/shared/db/schema/memories.ts
    src/shared/db/schema/index.ts
  </files>
  <action>
1. Create src/shared/db/schema/conversations.ts:
   ```typescript
   import { pgTable, uuid, text, timestamp, jsonb } from 'drizzle-orm/pg-core';
   import { organizations } from './organizations';
   import { projects } from './projects';
   import { users } from './users';

   export const conversations = pgTable('conversations', {
     id: uuid('id').primaryKey().defaultRandom(),
     organizationId: uuid('organization_id').notNull().references(() => organizations.id),
     userId: uuid('user_id').notNull().references(() => users.id),
     projectId: uuid('project_id').references(() => projects.id),  // Current project context
     title: text('title'),  // Auto-generated from first message
     metadata: jsonb('metadata'),
     createdAt: timestamp('created_at', { withTimezone: true }).defaultNow().notNull(),
     updatedAt: timestamp('updated_at', { withTimezone: true }).defaultNow().notNull(),
   });

   export const conversationMessages = pgTable('conversation_messages', {
     id: uuid('id').primaryKey().defaultRandom(),
     conversationId: uuid('conversation_id').notNull().references(() => conversations.id, { onDelete: 'cascade' }),
     role: text('role').notNull(),  // 'system', 'user', 'assistant'
     content: text('content').notNull(),
     metadata: jsonb('metadata'),  // Tool calls, function results, etc.
     createdAt: timestamp('created_at', { withTimezone: true }).defaultNow().notNull(),
   });

   export type Conversation = typeof conversations.$inferSelect;
   export type NewConversation = typeof conversations.$inferInsert;
   export type ConversationMessage = typeof conversationMessages.$inferSelect;
   export type NewConversationMessage = typeof conversationMessages.$inferInsert;
   ```

2. Create src/shared/db/schema/memories.ts:
   ```typescript
   import { pgTable, uuid, text, timestamp, integer, vector, jsonb } from 'drizzle-orm/pg-core';
   import { organizations } from './organizations';
   import { projects } from './projects';
   import { users } from './users';

   export const memories = pgTable('memories', {
     id: uuid('id').primaryKey().defaultRandom(),
     type: text('type').notNull(),  // 'working', 'episodic', 'semantic'
     scope: text('scope').notNull(),  // 'organization', 'project', 'user'
     organizationId: uuid('organization_id').notNull().references(() => organizations.id),
     projectId: uuid('project_id').references(() => projects.id),
     userId: uuid('user_id').references(() => users.id),
     conversationId: uuid('conversation_id'),  // Source conversation if applicable
     content: text('content').notNull(),
     embedding: vector('embedding', { dimensions: 1536 }),
     importance: integer('importance').default(5),  // 1-10 scale for retention priority
     metadata: jsonb('metadata'),
     createdAt: timestamp('created_at', { withTimezone: true }).defaultNow().notNull(),
     expiresAt: timestamp('expires_at', { withTimezone: true }),  // For working memory TTL
   });

   export type Memory = typeof memories.$inferSelect;
   export type NewMemory = typeof memories.$inferInsert;
   ```

3. Update src/shared/db/schema/index.ts to export new schemas

4. Generate and run migrations: `bun run db:generate && bun run db:migrate`

Per CONTEXT.md: "Create conversation_memories table with hierarchy support"
Per RESEARCH.md Pattern 3: Hierarchical Memory Tables
  </action>
  <verify>
    bun run db:migrate && bun run typecheck
  </verify>
  <done>Conversation and memory tables created with proper relationships</done>
</task>

<task type="auto">
  <name>Task 2: Create BullMQ Queue Infrastructure</name>
  <files>
    src/shared/lib/queue/client.ts
    src/shared/lib/queue/workers.ts
    src/shared/lib/queue/index.ts
    package.json
  </files>
  <action>
1. Install BullMQ and ioredis: `bun add bullmq ioredis`

2. Create src/shared/lib/queue/client.ts:
   ```typescript
   import { Queue } from 'bullmq';
   import IORedis from 'ioredis';
   import { env } from '../env';
   import { logger } from '../logger';

   // BullMQ requires ioredis, separate from the 'redis' package used elsewhere
   // Per RESEARCH.md Pitfall 6: Use separate connection for BullMQ
   let connection: IORedis | null = null;

   export function getQueueConnection(): IORedis {
     if (!connection) {
       connection = new IORedis(env.REDIS_URL, {
         maxRetriesPerRequest: null,  // Required by BullMQ
         enableReadyCheck: false,
       });
       connection.on('error', (err) => {
         logger.error({ err }, 'BullMQ Redis connection error');
       });
     }
     return connection;
   }

   export const memoryConsolidationQueue = new Queue('memory-consolidation', {
     connection: getQueueConnection(),
   });

   export const embeddingQueue = new Queue('embedding-generation', {
     connection: getQueueConnection(),
   });

   export async function closeQueueConnections(): Promise<void> {
     await memoryConsolidationQueue.close();
     await embeddingQueue.close();
     if (connection) {
       connection.disconnect();
       connection = null;
     }
   }
   ```

3. Create src/shared/lib/queue/workers.ts:
   ```typescript
   import { Worker } from 'bullmq';
   import { getQueueConnection } from './client';
   import { logger } from '../logger';

   const workers: Worker[] = [];

   export function registerWorker(worker: Worker): void {
     workers.push(worker);
     worker.on('failed', (job, err) => {
       logger.error({ jobId: job?.id, err }, 'Job failed');
     });
     worker.on('completed', (job) => {
       logger.info({ jobId: job.id }, 'Job completed');
     });
   }

   export async function closeAllWorkers(): Promise<void> {
     await Promise.all(workers.map(w => w.close()));
     workers.length = 0;
   }

   export { getQueueConnection };
   ```

4. Create src/shared/lib/queue/index.ts:
   ```typescript
   export { memoryConsolidationQueue, embeddingQueue, closeQueueConnections, getQueueConnection } from './client';
   export { registerWorker, closeAllWorkers } from './workers';
   ```
  </action>
  <verify>
    bun run typecheck
  </verify>
  <done>BullMQ queue infrastructure created with separate ioredis connection</done>
</task>

<task type="auto">
  <name>Task 3: Create Memory Service and Consolidation Worker</name>
  <files>
    src/features/memory/memory.types.ts
    src/features/memory/memory.service.ts
    src/features/memory/memory.worker.ts
    src/features/memory/index.ts
    src/index.ts
  </files>
  <action>
1. Create src/features/memory/memory.types.ts:
   ```typescript
   export type MemoryType = 'working' | 'episodic' | 'semantic';
   export type MemoryScope = 'organization' | 'project' | 'user';

   export type StoreMemoryInput = {
     type: MemoryType;
     scope: MemoryScope;
     organizationId: string;
     projectId?: string;
     userId?: string;
     conversationId?: string;
     content: string;
     importance?: number;
     expiresAt?: Date;
   };

   export type RetrieveMemoriesInput = {
     organizationId: string;
     projectId?: string;
     userId?: string;
     types?: MemoryType[];
     limit?: number;
   };

   // Capacity limits per CONTEXT.md: "Keep up to N entries, oldest removed when limit reached"
   export const MEMORY_CAPACITY = {
     organization: 1000,
     project: 500,
     user: 100,
   } as const;

   export const KEEP_RECENT_MESSAGES = 10;
   ```

2. Create src/features/memory/memory.service.ts:
   ```typescript
   import { and, asc, desc, eq, inArray, sql } from 'drizzle-orm';
   import { db } from '../../shared/db';
   import { memories } from '../../shared/db/schema';
   import { generateEmbedding } from '../llm';
   import type { StoreMemoryInput, RetrieveMemoriesInput, MemoryScope } from './memory.types';
   import { MEMORY_CAPACITY } from './memory.types';

   export async function storeMemory(input: StoreMemoryInput): Promise<string> {
     // Generate embedding for semantic search
     const embeddingResult = await generateEmbedding(input.content);

     const [inserted] = await db.insert(memories).values({
       type: input.type,
       scope: input.scope,
       organizationId: input.organizationId,
       projectId: input.projectId,
       userId: input.userId,
       conversationId: input.conversationId,
       content: input.content,
       embedding: embeddingResult.embedding,
       importance: input.importance ?? 5,
       expiresAt: input.expiresAt,
     }).returning({ id: memories.id });

     // Enforce capacity limits
     await enforceCapacityLimit(input.scope, input.organizationId, input.projectId, input.userId);

     return inserted.id;
   }

   async function enforceCapacityLimit(
     scope: MemoryScope,
     organizationId: string,
     projectId?: string,
     userId?: string
   ): Promise<void> {
     const capacity = MEMORY_CAPACITY[scope];

     // Build scope filter
     const scopeFilter = and(
       eq(memories.scope, scope),
       eq(memories.organizationId, organizationId),
       projectId ? eq(memories.projectId, projectId) : undefined,
       userId ? eq(memories.userId, userId) : undefined
     );

     // Count current entries
     const [{ count }] = await db
       .select({ count: sql<number>`count(*)::int` })
       .from(memories)
       .where(scopeFilter);

     if (count > capacity) {
       // Delete oldest entries exceeding capacity (keep by importance, then by date)
       const toDelete = await db
         .select({ id: memories.id })
         .from(memories)
         .where(scopeFilter)
         .orderBy(asc(memories.importance), asc(memories.createdAt))
         .limit(count - capacity);

       if (toDelete.length > 0) {
         await db.delete(memories).where(
           inArray(memories.id, toDelete.map(m => m.id))
         );
       }
     }
   }

   export async function retrieveMemories(input: RetrieveMemoriesInput) {
     const filters = [
       eq(memories.organizationId, input.organizationId),
     ];

     if (input.projectId) filters.push(eq(memories.projectId, input.projectId));
     if (input.userId) filters.push(eq(memories.userId, input.userId));
     if (input.types?.length) filters.push(inArray(memories.type, input.types));

     return db
       .select()
       .from(memories)
       .where(and(...filters))
       .orderBy(desc(memories.importance), desc(memories.createdAt))
       .limit(input.limit ?? 20);
   }

   export async function deleteExpiredMemories(): Promise<number> {
     const result = await db
       .delete(memories)
       .where(sql`${memories.expiresAt} < NOW()`);
     return result.rowCount ?? 0;
   }
   ```

3. Create src/features/memory/memory.worker.ts:
   ```typescript
   import { Worker, Job } from 'bullmq';
   import { and, asc, eq } from 'drizzle-orm';
   import { db } from '../../shared/db';
   import { conversationMessages } from '../../shared/db/schema';
   import { chatCompletion } from '../llm';
   import { storeMemory } from './memory.service';
   import { getQueueConnection, registerWorker } from '../../shared/lib/queue';
   import { logger } from '../../shared/lib/logger';
   import { KEEP_RECENT_MESSAGES } from './memory.types';

   type ConsolidationJobData = {
     conversationId: string;
     organizationId: string;
     projectId?: string;
     userId: string;
   };

   async function consolidateConversation(job: Job<ConsolidationJobData>) {
     const { conversationId, organizationId, projectId, userId } = job.data;

     // Get all messages in conversation
     const messages = await db
       .select()
       .from(conversationMessages)
       .where(eq(conversationMessages.conversationId, conversationId))
       .orderBy(asc(conversationMessages.createdAt));

     if (messages.length <= KEEP_RECENT_MESSAGES) {
       logger.info({ conversationId }, 'Not enough messages to consolidate');
       return;
     }

     // Split into messages to summarize and messages to keep
     const toSummarize = messages.slice(0, -KEEP_RECENT_MESSAGES);
     const toKeep = messages.slice(-KEEP_RECENT_MESSAGES);

     // Generate summary using LLM
     const summaryPrompt = [
       { role: 'system' as const, content: 'Summarize this conversation concisely, preserving key facts, decisions, and action items. Focus on information that would be useful in future conversations.' },
       ...toSummarize.map(m => ({
         role: m.role as 'user' | 'assistant' | 'system',
         content: m.content,
       })),
     ];

     const summaryResult = await chatCompletion(summaryPrompt);

     // Store summary as semantic memory
     await storeMemory({
       type: 'semantic',
       scope: projectId ? 'project' : 'user',
       organizationId,
       projectId,
       userId,
       conversationId,
       content: summaryResult.content,
       importance: 7,  // Summaries are moderately important
     });

     // Delete summarized messages
     await db
       .delete(conversationMessages)
       .where(
         and(
           eq(conversationMessages.conversationId, conversationId),
           sql`${conversationMessages.id} IN (${toSummarize.map(m => `'${m.id}'`).join(',')})`
         )
       );

     logger.info({
       conversationId,
       summarized: toSummarize.length,
       kept: toKeep.length,
     }, 'Conversation consolidated');
   }

   export function startMemoryConsolidationWorker() {
     const worker = new Worker<ConsolidationJobData>(
       'memory-consolidation',
       consolidateConversation,
       {
         connection: getQueueConnection(),
         concurrency: 2,
         limiter: {
           max: 10,
           duration: 60000,  // Max 10 consolidations per minute
         },
       }
     );

     registerWorker(worker);
     logger.info('Memory consolidation worker started');
     return worker;
   }
   ```

4. Create src/features/memory/index.ts:
   ```typescript
   export * from './memory.types';
   export { storeMemory, retrieveMemories, deleteExpiredMemories } from './memory.service';
   export { startMemoryConsolidationWorker } from './memory.worker';
   ```

5. Update src/index.ts to start the worker:
   - Import { startMemoryConsolidationWorker } from './features/memory'
   - Call startMemoryConsolidationWorker() after server starts
   - Add cleanup in graceful shutdown: closeAllWorkers()

Per CONTEXT.md: "Background job to summarize old messages"
Per RESEARCH.md Pattern 5: Rate-Limited LLM Queue
  </action>
  <verify>
    bun run typecheck
  </verify>
  <done>Memory service with capacity limits and background consolidation worker implemented</done>
</task>

</tasks>

<verification>
1. Schema: conversations, conversation_messages, memories tables exist
2. TypeScript: `bun run typecheck` passes
3. Worker: Memory consolidation worker starts without errors
4. Queue: BullMQ queues created successfully
</verification>

<success_criteria>
- Conversations and messages stored in database
- Memories support hierarchical scopes (org/project/user)
- Capacity limits enforced per scope
- Memory consolidation worker summarizes old messages
- BullMQ with separate ioredis connection (not mixing with existing redis client)
- Worker rate-limited to prevent LLM API overload
- Graceful shutdown closes workers and connections
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-infrastructure/03-03-SUMMARY.md`
</output>
